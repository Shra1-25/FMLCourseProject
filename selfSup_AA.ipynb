{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# !pip3 install --force-reinstall git+https://github.com/fra31/auto-attack"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "cd /scratch/snx3000/pchaudha/Shravan/FML/SelfSupDefense/ "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/scratch/snx3000/pchaudha/Shravan/FML/SelfSupDefense\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "''''If you see bad robustness, it is due to use the wrong normalization in L26-32'''\n",
    "import sys\n",
    "import math, time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "\n",
    "from learning.wideresnet import WideResNet, WideResNetBD, WideResNetMed_SSL, WRN34_out_branch\n",
    "from learning.preactresnet import PreActResNet18Mhead, Res18_out3_model,Res18_out4_model,Res18_out5_model,Res18_out6_model\n",
    "from data.gaussian_blur import GaussianLayer\n",
    "\n",
    "from utils import *\n",
    "from cifar10_defense_rebAA import *\n",
    "from Qmodels.wide_resnet import *\n",
    "from Qmodels.utils import *"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "head num 8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "mu = torch.tensor(cifar10_mean).view(3,1,1).cuda()\n",
    "std = torch.tensor(cifar10_std).view(3,1,1).cuda()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model', default='WideResNet')  #WideResNet\n",
    "    parser.add_argument('--l2', default=0, type=float)\n",
    "    parser.add_argument('--l1', default=0, type=float)\n",
    "    parser.add_argument('--batch-size', default=1024, type=int)\n",
    "    parser.add_argument('--contrastive_bs', default=512, type=int)\n",
    "    parser.add_argument('--data-dir', default='../cifar-data', type=str)\n",
    "    parser.add_argument('--epochs', default=200, type=int)\n",
    "    parser.add_argument('--lr-schedule', default='piecewise', choices=['superconverge', 'piecewise', 'linear', 'piecewisesmoothed', 'piecewisezoom', 'onedrop', 'multipledecay', 'cosine'])\n",
    "    parser.add_argument('--lr-max', default=0.1, type=float)\n",
    "    parser.add_argument('--lr-one-drop', default=0.01, type=float)\n",
    "    parser.add_argument('--lam_res', default=1, type=float)\n",
    "    parser.add_argument('--adda_times', default=1, type=float)\n",
    "    parser.add_argument('--lr-drop-epoch', default=100, type=int)\n",
    "    parser.add_argument('--attack', default='pgd', type=str, choices=['pgd', 'fgsm', 'free', 'none'])\n",
    "    parser.add_argument('--epsilon', default=8, type=int)\n",
    "    parser.add_argument('--attack-iters', default=10, type=int)\n",
    "    parser.add_argument('--bd_attack_iters', default=4, type=int)\n",
    "    parser.add_argument('--restarts', default=1, type=int)\n",
    "    parser.add_argument('--neg_size', default=10, type=int)\n",
    "    parser.add_argument('--pgd-alpha', default=2, type=float)\n",
    "    parser.add_argument('--fgsm-alpha', default=1.25, type=float)\n",
    "    parser.add_argument('--norm', default='l_inf', type=str, choices=['l_inf', 'l_2', 'l_1'])\n",
    "    parser.add_argument('--fgsm-init', default='random', choices=['zero', 'random', 'previous'])\n",
    "    parser.add_argument('--fname', default='train_ssl', type=str)\n",
    "    import socket\n",
    "    # if 'cv10' in socket.gethostname():\n",
    "    #     parser.add_argument('--save_root_path', default='/local/vondrick/chengzhi/SSRobust', type=str)\n",
    "    if 'cv' in socket.gethostname():\n",
    "        parser.add_argument('--save_root_path', default='/scratch/snx3000/pchaudha/Shravan/FML/SelfSupDefense/', type=str)\n",
    "    else:\n",
    "        parser.add_argument('--save_root_path', default='/scratch/snx3000/pchaudha/Shravan/FML/SelfSupDefense/', type=str)\n",
    "\n",
    "    parser.add_argument('--ssl_model_path', default='', type=str)\n",
    "    parser.add_argument('--attack_type', default='', type=str)\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--half', action='store_true')\n",
    "    parser.add_argument('--width-factor', default=10, type=int)\n",
    "    parser.add_argument('--constrastive_head', default=16, type=int)\n",
    "    parser.add_argument('--md_path', default='', type=str)\n",
    "    parser.add_argument('--cutout', action='store_true')\n",
    "    parser.add_argument('--cutout-len', type=int)\n",
    "    parser.add_argument('--mixup', action='store_true')\n",
    "    parser.add_argument('--rand', action='store_true')\n",
    "\n",
    "    parser.add_argument('--debug', action='store_true')\n",
    "    parser.add_argument('--MCtimes', default=1, type=int)\n",
    "    parser.add_argument('--n_views', default=4, type=int)\n",
    "    parser.add_argument('--eval_freq', default=10, type=int)\n",
    "    parser.add_argument('--mixup-alpha', type=float)\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--val', action='store_true')\n",
    "    parser.add_argument('--carmon', action='store_true')\n",
    "    parser.add_argument('--TRADES', action='store_true')\n",
    "    parser.add_argument('--Bag', action='store_true')\n",
    "    parser.add_argument('--res18', action='store_true')\n",
    "    parser.add_argument('--new', action='store_true')\n",
    "    parser.add_argument('--eval_only', action='store_true')\n",
    "    parser.add_argument('--random_noise', action='store_true')\n",
    "    parser.add_argument('--foolbox', action='store_true')\n",
    "    return parser.parse_known_args()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print(\"eval only\")\n",
    "adda_times=1\n",
    "\n",
    "args, _ = get_args()\n",
    "import uuid\n",
    "import datetime\n",
    "unique_str = str(uuid.uuid4())[:8]\n",
    "timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H:%M:%S')\n",
    "\n",
    "args.fname = os.path.join(args.save_root_path, args.fname, timestamp + unique_str)\n",
    "\n",
    "args.fname = 'test'\n",
    "args.md_path = '/scratch/snx3000/pchaudha/Shravan/FML/SelfSupDefense/Qmodels/model_wide_cifar10.pth'\n",
    "args.carmon = True\n",
    "args.eval_only = True\n",
    "args.ssl_model_path = '/scratch/snx3000/pchaudha/Shravan/FML/SelfSupDefense/checkpoints/ssl_model_130.pth'\n",
    "args.contrastive_bs = 128\n",
    "args.batch_size = 128\n",
    "\n",
    "\n",
    "if not os.path.exists(args.fname):\n",
    "    os.makedirs(args.fname)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format='[%(asctime)s] - %(message)s',\n",
    "    datefmt='%Y/%m/%d %H:%M:%S',\n",
    "    level=logging.DEBUG,\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(args.fname, 'eval.log' if args.eval else 'output.log')),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "\n",
    "logger.info(args)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[2022/05/04 01:41:52] - Namespace(model='WideResNet', l2=0, l1=0, batch_size=128, contrastive_bs=128, data_dir='../cifar-data', epochs=200, lr_schedule='piecewise', lr_max=0.1, lr_one_drop=0.01, lam_res=1, adda_times=1, lr_drop_epoch=100, attack='pgd', epsilon=8, attack_iters=10, bd_attack_iters=4, restarts=1, neg_size=10, pgd_alpha=2, fgsm_alpha=1.25, norm='l_inf', fgsm_init='random', fname='test', save_root_path='/scratch/snx3000/pchaudha/Shravan/FML/SelfSupDefense/', ssl_model_path='/scratch/snx3000/pchaudha/Shravan/FML/SelfSupDefense/checkpoints/ssl_model_130.pth', attack_type='', seed=0, half=False, width_factor=10, constrastive_head=16, md_path='/scratch/snx3000/pchaudha/Shravan/FML/SelfSupDefense/Qmodels/model_wide_cifar10.pth', cutout=False, cutout_len=None, mixup=False, rand=False, debug=False, MCtimes=1, n_views=4, eval_freq=10, mixup_alpha=None, eval=False, val=False, carmon=True, TRADES=False, Bag=False, res18=False, new=False, eval_only=True, random_noise=False, foolbox=False)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "eval only\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "transforms = [Crop(32, 32), FlipLR()]\n",
    "dataset = cifar10(args.data_dir)\n",
    "\n",
    "train_set = list(zip(transpose(pad(dataset['train']['data'], 4) / 255.),\n",
    "                     dataset['train']['labels']))\n",
    "train_set_x = Transform(train_set, transforms)\n",
    "train_batches = Batches(train_set_x, args.batch_size, shuffle=True, set_random_choices=True, num_workers=2)\n",
    "\n",
    "test_set = list(zip(transpose(dataset['test']['data'] / 255.), dataset['test']['labels']))\n",
    "test_batches = Batches(test_set, args.batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "epsilon = (args.epsilon / 255.)\n",
    "pgd_alpha = (args.pgd_alpha / 255.)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# if args.model == 'PreActResNet18' or args.res18:\n",
    "#     from learning.preactresnet import PreActResNet18SSL, PreActResNet18\n",
    "#     model = PreActResNet18SSL() # TODO: make it single head?\n",
    "#     c_head_model = Res18_out6_model()\n",
    "\n",
    "\n",
    "# elif args.model == 'WideResNet':\n",
    "#     # model = WideResNetMed_SSL(34, 10, widen_factor=args.width_factor, dropRate=0.0)\n",
    "#     if args.carmon:\n",
    "#         from learning.unlabel_WRN import WideResNet_2\n",
    "#         model = WideResNet_2(depth=28, widen_factor=10)\n",
    "#     elif args.TRADES or args.Bag:\n",
    "#         from learning.unlabel_WRN import WideResNet_2\n",
    "#         model = WideResNet_2(depth=34, widen_factor=10)\n",
    "\n",
    "#     else:\n",
    "#         model = WideResNetMed_SSL(34, 10, widen_factor=args.width_factor, dropRate=0.0)\n",
    "#         # print(model)\n",
    "#     c_head_model = WRN34_out_branch()\n",
    "\n",
    "# else:\n",
    "#     raise ValueError(\"Unknown model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from Qmodels.utils import (upper_limit, lower_limit, std, mu, clamp, get_loaders,\n",
    "    attack_pgd, evaluate_pgd, evaluate_standard)\n",
    "from Qmodels.wide_resnet import WideResNet32\n",
    "num_bits_list = [4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "_, _, dataset_normalization = get_loaders(args.data_dir, args.batch_size, dataset='cifar10', norm=False)\n",
    "model = WideResNet32(num_bits_list, 10, normalize=dataset_normalization)\n",
    "model = torch.nn.DataParallel(model)\n",
    "pretrained_model = torch.load(\"Qmodels/model_wide_cifar10.pth\")\n",
    "partial = pretrained_model['state_dict']\n",
    "model.load_state_dict(partial, strict=False)\n",
    "c_head_model = WRN34_out_branch()\n",
    "_ = model.cuda()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from torchsummary import summary\n",
    "model.module.set_precision(num_bits=np.random.choice(num_bits_list), num_grad_bits=0)\n",
    "summary(model, (3, 32, 32))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "NormalizeByChannelMeanStd-1            [-1, 3, 32, 32]               0\n",
      "      QuantMeasure-2            [-1, 3, 32, 32]               0\n",
      "           QConv2d-3           [-1, 16, 32, 32]             432\n",
      "       BatchNorm2d-4           [-1, 16, 32, 32]              32\n",
      "     USBatchNorm2d-5           [-1, 16, 32, 32]              32\n",
      "              ReLU-6           [-1, 16, 32, 32]               0\n",
      "      QuantMeasure-7           [-1, 16, 32, 32]               0\n",
      "           QConv2d-8          [-1, 160, 32, 32]          23,040\n",
      "       BatchNorm2d-9          [-1, 160, 32, 32]             320\n",
      "    USBatchNorm2d-10          [-1, 160, 32, 32]             320\n",
      "             ReLU-11          [-1, 160, 32, 32]               0\n",
      "     QuantMeasure-12          [-1, 160, 32, 32]               0\n",
      "          QConv2d-13          [-1, 160, 32, 32]         230,400\n",
      "     QuantMeasure-14           [-1, 16, 32, 32]               0\n",
      "          QConv2d-15          [-1, 160, 32, 32]           2,560\n",
      "       BasicBlock-16          [-1, 160, 32, 32]               0\n",
      "      BatchNorm2d-17          [-1, 160, 32, 32]             320\n",
      "    USBatchNorm2d-18          [-1, 160, 32, 32]             320\n",
      "             ReLU-19          [-1, 160, 32, 32]               0\n",
      "     QuantMeasure-20          [-1, 160, 32, 32]               0\n",
      "          QConv2d-21          [-1, 160, 32, 32]         230,400\n",
      "      BatchNorm2d-22          [-1, 160, 32, 32]             320\n",
      "    USBatchNorm2d-23          [-1, 160, 32, 32]             320\n",
      "             ReLU-24          [-1, 160, 32, 32]               0\n",
      "     QuantMeasure-25          [-1, 160, 32, 32]               0\n",
      "          QConv2d-26          [-1, 160, 32, 32]         230,400\n",
      "       BasicBlock-27          [-1, 160, 32, 32]               0\n",
      "      BatchNorm2d-28          [-1, 160, 32, 32]             320\n",
      "    USBatchNorm2d-29          [-1, 160, 32, 32]             320\n",
      "             ReLU-30          [-1, 160, 32, 32]               0\n",
      "     QuantMeasure-31          [-1, 160, 32, 32]               0\n",
      "          QConv2d-32          [-1, 160, 32, 32]         230,400\n",
      "      BatchNorm2d-33          [-1, 160, 32, 32]             320\n",
      "    USBatchNorm2d-34          [-1, 160, 32, 32]             320\n",
      "             ReLU-35          [-1, 160, 32, 32]               0\n",
      "     QuantMeasure-36          [-1, 160, 32, 32]               0\n",
      "          QConv2d-37          [-1, 160, 32, 32]         230,400\n",
      "       BasicBlock-38          [-1, 160, 32, 32]               0\n",
      "      BatchNorm2d-39          [-1, 160, 32, 32]             320\n",
      "    USBatchNorm2d-40          [-1, 160, 32, 32]             320\n",
      "             ReLU-41          [-1, 160, 32, 32]               0\n",
      "     QuantMeasure-42          [-1, 160, 32, 32]               0\n",
      "          QConv2d-43          [-1, 160, 32, 32]         230,400\n",
      "      BatchNorm2d-44          [-1, 160, 32, 32]             320\n",
      "    USBatchNorm2d-45          [-1, 160, 32, 32]             320\n",
      "             ReLU-46          [-1, 160, 32, 32]               0\n",
      "     QuantMeasure-47          [-1, 160, 32, 32]               0\n",
      "          QConv2d-48          [-1, 160, 32, 32]         230,400\n",
      "       BasicBlock-49          [-1, 160, 32, 32]               0\n",
      "      BatchNorm2d-50          [-1, 160, 32, 32]             320\n",
      "    USBatchNorm2d-51          [-1, 160, 32, 32]             320\n",
      "             ReLU-52          [-1, 160, 32, 32]               0\n",
      "     QuantMeasure-53          [-1, 160, 32, 32]               0\n",
      "          QConv2d-54          [-1, 160, 32, 32]         230,400\n",
      "      BatchNorm2d-55          [-1, 160, 32, 32]             320\n",
      "    USBatchNorm2d-56          [-1, 160, 32, 32]             320\n",
      "             ReLU-57          [-1, 160, 32, 32]               0\n",
      "     QuantMeasure-58          [-1, 160, 32, 32]               0\n",
      "          QConv2d-59          [-1, 160, 32, 32]         230,400\n",
      "       BasicBlock-60          [-1, 160, 32, 32]               0\n",
      "     NetworkBlock-61          [-1, 160, 32, 32]               0\n",
      "      BatchNorm2d-62          [-1, 160, 32, 32]             320\n",
      "    USBatchNorm2d-63          [-1, 160, 32, 32]             320\n",
      "             ReLU-64          [-1, 160, 32, 32]               0\n",
      "     QuantMeasure-65          [-1, 160, 32, 32]               0\n",
      "          QConv2d-66          [-1, 320, 16, 16]         460,800\n",
      "      BatchNorm2d-67          [-1, 320, 16, 16]             640\n",
      "    USBatchNorm2d-68          [-1, 320, 16, 16]             640\n",
      "             ReLU-69          [-1, 320, 16, 16]               0\n",
      "     QuantMeasure-70          [-1, 320, 16, 16]               0\n",
      "          QConv2d-71          [-1, 320, 16, 16]         921,600\n",
      "     QuantMeasure-72          [-1, 160, 32, 32]               0\n",
      "          QConv2d-73          [-1, 320, 16, 16]          51,200\n",
      "       BasicBlock-74          [-1, 320, 16, 16]               0\n",
      "      BatchNorm2d-75          [-1, 320, 16, 16]             640\n",
      "    USBatchNorm2d-76          [-1, 320, 16, 16]             640\n",
      "             ReLU-77          [-1, 320, 16, 16]               0\n",
      "     QuantMeasure-78          [-1, 320, 16, 16]               0\n",
      "          QConv2d-79          [-1, 320, 16, 16]         921,600\n",
      "      BatchNorm2d-80          [-1, 320, 16, 16]             640\n",
      "    USBatchNorm2d-81          [-1, 320, 16, 16]             640\n",
      "             ReLU-82          [-1, 320, 16, 16]               0\n",
      "     QuantMeasure-83          [-1, 320, 16, 16]               0\n",
      "          QConv2d-84          [-1, 320, 16, 16]         921,600\n",
      "       BasicBlock-85          [-1, 320, 16, 16]               0\n",
      "      BatchNorm2d-86          [-1, 320, 16, 16]             640\n",
      "    USBatchNorm2d-87          [-1, 320, 16, 16]             640\n",
      "             ReLU-88          [-1, 320, 16, 16]               0\n",
      "     QuantMeasure-89          [-1, 320, 16, 16]               0\n",
      "          QConv2d-90          [-1, 320, 16, 16]         921,600\n",
      "      BatchNorm2d-91          [-1, 320, 16, 16]             640\n",
      "    USBatchNorm2d-92          [-1, 320, 16, 16]             640\n",
      "             ReLU-93          [-1, 320, 16, 16]               0\n",
      "     QuantMeasure-94          [-1, 320, 16, 16]               0\n",
      "          QConv2d-95          [-1, 320, 16, 16]         921,600\n",
      "       BasicBlock-96          [-1, 320, 16, 16]               0\n",
      "      BatchNorm2d-97          [-1, 320, 16, 16]             640\n",
      "    USBatchNorm2d-98          [-1, 320, 16, 16]             640\n",
      "             ReLU-99          [-1, 320, 16, 16]               0\n",
      "    QuantMeasure-100          [-1, 320, 16, 16]               0\n",
      "         QConv2d-101          [-1, 320, 16, 16]         921,600\n",
      "     BatchNorm2d-102          [-1, 320, 16, 16]             640\n",
      "   USBatchNorm2d-103          [-1, 320, 16, 16]             640\n",
      "            ReLU-104          [-1, 320, 16, 16]               0\n",
      "    QuantMeasure-105          [-1, 320, 16, 16]               0\n",
      "         QConv2d-106          [-1, 320, 16, 16]         921,600\n",
      "      BasicBlock-107          [-1, 320, 16, 16]               0\n",
      "     BatchNorm2d-108          [-1, 320, 16, 16]             640\n",
      "   USBatchNorm2d-109          [-1, 320, 16, 16]             640\n",
      "            ReLU-110          [-1, 320, 16, 16]               0\n",
      "    QuantMeasure-111          [-1, 320, 16, 16]               0\n",
      "         QConv2d-112          [-1, 320, 16, 16]         921,600\n",
      "     BatchNorm2d-113          [-1, 320, 16, 16]             640\n",
      "   USBatchNorm2d-114          [-1, 320, 16, 16]             640\n",
      "            ReLU-115          [-1, 320, 16, 16]               0\n",
      "    QuantMeasure-116          [-1, 320, 16, 16]               0\n",
      "         QConv2d-117          [-1, 320, 16, 16]         921,600\n",
      "      BasicBlock-118          [-1, 320, 16, 16]               0\n",
      "    NetworkBlock-119          [-1, 320, 16, 16]               0\n",
      "     BatchNorm2d-120          [-1, 320, 16, 16]             640\n",
      "   USBatchNorm2d-121          [-1, 320, 16, 16]             640\n",
      "            ReLU-122          [-1, 320, 16, 16]               0\n",
      "    QuantMeasure-123          [-1, 320, 16, 16]               0\n",
      "         QConv2d-124            [-1, 640, 8, 8]       1,843,200\n",
      "     BatchNorm2d-125            [-1, 640, 8, 8]           1,280\n",
      "   USBatchNorm2d-126            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-127            [-1, 640, 8, 8]               0\n",
      "    QuantMeasure-128            [-1, 640, 8, 8]               0\n",
      "         QConv2d-129            [-1, 640, 8, 8]       3,686,400\n",
      "    QuantMeasure-130          [-1, 320, 16, 16]               0\n",
      "         QConv2d-131            [-1, 640, 8, 8]         204,800\n",
      "      BasicBlock-132            [-1, 640, 8, 8]               0\n",
      "     BatchNorm2d-133            [-1, 640, 8, 8]           1,280\n",
      "   USBatchNorm2d-134            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-135            [-1, 640, 8, 8]               0\n",
      "    QuantMeasure-136            [-1, 640, 8, 8]               0\n",
      "         QConv2d-137            [-1, 640, 8, 8]       3,686,400\n",
      "     BatchNorm2d-138            [-1, 640, 8, 8]           1,280\n",
      "   USBatchNorm2d-139            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-140            [-1, 640, 8, 8]               0\n",
      "    QuantMeasure-141            [-1, 640, 8, 8]               0\n",
      "         QConv2d-142            [-1, 640, 8, 8]       3,686,400\n",
      "      BasicBlock-143            [-1, 640, 8, 8]               0\n",
      "     BatchNorm2d-144            [-1, 640, 8, 8]           1,280\n",
      "   USBatchNorm2d-145            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-146            [-1, 640, 8, 8]               0\n",
      "    QuantMeasure-147            [-1, 640, 8, 8]               0\n",
      "         QConv2d-148            [-1, 640, 8, 8]       3,686,400\n",
      "     BatchNorm2d-149            [-1, 640, 8, 8]           1,280\n",
      "   USBatchNorm2d-150            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-151            [-1, 640, 8, 8]               0\n",
      "    QuantMeasure-152            [-1, 640, 8, 8]               0\n",
      "         QConv2d-153            [-1, 640, 8, 8]       3,686,400\n",
      "      BasicBlock-154            [-1, 640, 8, 8]               0\n",
      "     BatchNorm2d-155            [-1, 640, 8, 8]           1,280\n",
      "   USBatchNorm2d-156            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-157            [-1, 640, 8, 8]               0\n",
      "    QuantMeasure-158            [-1, 640, 8, 8]               0\n",
      "         QConv2d-159            [-1, 640, 8, 8]       3,686,400\n",
      "     BatchNorm2d-160            [-1, 640, 8, 8]           1,280\n",
      "   USBatchNorm2d-161            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-162            [-1, 640, 8, 8]               0\n",
      "    QuantMeasure-163            [-1, 640, 8, 8]               0\n",
      "         QConv2d-164            [-1, 640, 8, 8]       3,686,400\n",
      "      BasicBlock-165            [-1, 640, 8, 8]               0\n",
      "     BatchNorm2d-166            [-1, 640, 8, 8]           1,280\n",
      "   USBatchNorm2d-167            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-168            [-1, 640, 8, 8]               0\n",
      "    QuantMeasure-169            [-1, 640, 8, 8]               0\n",
      "         QConv2d-170            [-1, 640, 8, 8]       3,686,400\n",
      "     BatchNorm2d-171            [-1, 640, 8, 8]           1,280\n",
      "   USBatchNorm2d-172            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-173            [-1, 640, 8, 8]               0\n",
      "    QuantMeasure-174            [-1, 640, 8, 8]               0\n",
      "         QConv2d-175            [-1, 640, 8, 8]       3,686,400\n",
      "      BasicBlock-176            [-1, 640, 8, 8]               0\n",
      "    NetworkBlock-177            [-1, 640, 8, 8]               0\n",
      "     BatchNorm2d-178            [-1, 640, 8, 8]           1,280\n",
      "   USBatchNorm2d-179            [-1, 640, 8, 8]           1,280\n",
      "            ReLU-180            [-1, 640, 8, 8]               0\n",
      "          Linear-181                   [-1, 10]           6,410\n",
      "      WideResNet-182      [[-1, 10], [-1, 640]]               0\n",
      "================================================================\n",
      "Total params: 46,182,906\n",
      "Trainable params: 46,182,906\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 127.00\n",
      "Params size (MB): 176.17\n",
      "Estimated Total Size (MB): 303.18\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# if not args.TRADES and not args.Bag:\n",
    "    # model = nn.DataParallel(model).cuda()\n",
    "c_head_model = nn.DataParallel(c_head_model).cuda()\n",
    "c_head_model.train()\n",
    "\n",
    "\n",
    "if args.l2:\n",
    "    decay, no_decay = [], []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bn' not in name and 'bias' not in name:\n",
    "            decay.append(param)\n",
    "        else:\n",
    "            no_decay.append(param)\n",
    "    params_bkbone = [{'params': decay, 'weight_decay': args.l2},\n",
    "              {'params': no_decay, 'weight_decay': 0}]\n",
    "else:\n",
    "    # params_bkbone = model.parameters()\n",
    "    decay, no_decay = [], []\n",
    "    for name, param in c_head_model.named_parameters():\n",
    "        if 'bn' not in name and 'bias' not in name:\n",
    "            decay.append(param)\n",
    "        else:\n",
    "            no_decay.append(param)\n",
    "    params = [{'params': decay, 'weight_decay': args.l2},\n",
    "                     {'params': no_decay, 'weight_decay': 0}]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def lr_schedule(t):\n",
    "    if t / args.epochs < 0.5:\n",
    "        return args.lr_max\n",
    "    elif t / args.epochs < 0.75:\n",
    "        return args.lr_max / 10.\n",
    "    else:\n",
    "        return args.lr_max / 100."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "learning_rate=1e-4\n",
    "opt = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "s = 1\n",
    "size = 32"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from torchvision.transforms import transforms\n",
    "# color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
    "transforms = torch.nn.Sequential(\n",
    "    transforms.RandomResizedCrop(size=size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomApply([color_jitter], p=0.8),\n",
    "    transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    # GaussianBlur(kernel_size=int(0.1 * size)),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "test_robust_loss = 0\n",
    "test_robust_acc = 0\n",
    "contrastive_attack_loss = 0\n",
    "contrastive_clean_loss = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "if args.res18:\n",
    "    # import pdb; pdb.set_trace()\n",
    "    if args.new:\n",
    "        tmp = torch.load(args.ssl_model_path)['ssl_model']\n",
    "    else:\n",
    "        tmp = torch.load(args.ssl_model_path)\n",
    "else:\n",
    "    tmp = torch.load(args.ssl_model_path)['ssl_model']\n",
    "c_head_model.load_state_dict(tmp)\n",
    "\n",
    "c_head_model.eval()\n",
    "\n",
    "\n",
    "for i, batch in enumerate(train_batches):\n",
    "    model.module.set_precision(num_bits=np.random.choice(num_bits_list), num_grad_bits=0)\n",
    "    X, y = batch['input'], batch['target']\n",
    "    model.module.set_precision(num_bits=np.random.choice(num_bits_list), num_grad_bits=0)\n",
    "    contrastive_Loss = \\\n",
    "        calculate_contrastive_Mhead_loss(X, scripted_transforms, model, criterion, c_head_model)\n",
    "    break\n",
    "    \n",
    "# from autoattack import AutoAttack\n",
    "from AAattack.autoattack import AutoAttack\n",
    "adversary = AutoAttack(model, normalize, norm='Linf', eps=epsilon, log_path='./log/tmp.txt')\n",
    "adversary.attacks_to_run = ['apgd-ce', 'apgd-t', 'fab-t', 'square']\n",
    "adversary.apgd.n_restarts = 2\n",
    "adversary.fab.n_restarts = 2\n",
    "\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "test_robust_loss = 0\n",
    "test_robust_acc = 0\n",
    "db_rob_acc_all=0\n",
    "TestX = []\n",
    "TestY = []\n",
    "Testdelta = []\n",
    "test_n = 0\n",
    "\n",
    "db_test_acc_clean_all=0\n",
    "\n",
    "print('epsilon', epsilon)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "setting parameters for standard version\n",
      "epsilon 0.03137254901960784\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "contrastive_attack_loss=0\n",
    "contrastive_clean_loss=0\n",
    "for i, batch in enumerate(test_batches):\n",
    "    if i>9:#args.debug and i > 0:\n",
    "        break\n",
    "    \n",
    "    X, y = batch['input'], batch['target']\n",
    "    TestX.append(X)\n",
    "    TestY.append(y)\n",
    "\n",
    "    X = X.cuda()\n",
    "    y = y.cuda()\n",
    "\n",
    "    # Generate attack with AutoAttack\n",
    "    adv_complete = adversary.run_standard_evaluation(X, y, bs=X.size(0))\n",
    "    print('low', adv_complete.min(), adv_complete.max())\n",
    "    delta = adv_complete - X\n",
    "    print('delta', delta.min(), delta.max())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        db_predict = model(normalize(adv_complete))[0]\n",
    "        db_test_acc = (db_predict.max(1)[1] == y).sum().item()\n",
    "        db_rob_acc_all += db_test_acc\n",
    "        n = y.size(0)\n",
    "\n",
    "        db_predict_clean = model(normalize(X))[0]\n",
    "        db_test_acc_clean_all += (db_predict_clean.max(1)[1] == y).sum().item()\n",
    "        print(\"db rob acc \", db_test_acc/n, '\\n\\n\\n\\n')\n",
    "\n",
    "    # TODO: the saved adv is weaker than reported in AA paper, maybe of because rounding error?\n",
    "\n",
    "    delta = delta.detach()\n",
    "\n",
    "    Testdelta.append(delta)\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        robust_output, _ = model(\n",
    "            normalize(torch.clamp(X + delta[:X.size(0)], min=lower_limit, max=upper_limit)))\n",
    "        output, _ = model(normalize(X))\n",
    "\n",
    "        robust_loss = criterion(robust_output, y)\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        Adv_image = torch.clamp(X + delta[:X.size(0)], min=lower_limit, max=upper_limit)\n",
    "        contrastive_attack = \\\n",
    "            calculate_contrastive_Mhead_loss(Adv_image, scripted_transforms, model, criterion, c_head_model, n_views=4)\n",
    "        contrastive_clean = \\\n",
    "            calculate_contrastive_Mhead_loss(X, scripted_transforms, model, criterion, c_head_model, n_views=4)\n",
    "\n",
    "    contrastive_attack_loss += contrastive_attack.item() * y.size(0)\n",
    "    contrastive_clean_loss += contrastive_clean.item() * y.size(0)\n",
    "\n",
    "    test_robust_loss += robust_loss.item() * y.size(0)\n",
    "    test_robust_acc += (robust_output.max(1)[1] == y).sum().item()\n",
    "    test_loss += loss.item() * y.size(0)\n",
    "    test_acc += (output.max(1)[1] == y).sum().item()\n",
    "    test_n += y.size(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"test_robust_acc\", test_robust_acc/test_n, db_rob_acc_all/test_n, 'clean', db_test_acc_clean_all/test_n)\n",
    "\n",
    "print('clean contrastive=%.6f \\t adv contrastive=%.6f' %\n",
    "                    ((contrastive_clean_loss / test_n), (contrastive_attack_loss / test_n)))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using standard version including apgd-ce, apgd-t, fab-t, square\n",
      "initial accuracy: 82.03%\n",
      "apgd-ce - 1/1 - 38 out of 105 successfully perturbed\n",
      "robust accuracy after APGD-CE: 52.34% (total time 170.0 s)\n",
      "apgd-t - 1/1 - 6 out of 67 successfully perturbed\n",
      "robust accuracy after APGD-T: 47.66% (total time 752.9 s)\n",
      "fab-t - 1/1 - 0 out of 61 successfully perturbed\n",
      "robust accuracy after FAB-T: 47.66% (total time 1485.5 s)\n",
      "square - 1/1 - 0 out of 61 successfully perturbed\n",
      "robust accuracy after SQUARE: 47.66% (total time 2341.0 s)\n",
      "max Linf perturbation: 0.03137, nan in tensor: 0, max: 1.00000, min: 0.00000\n",
      "robust accuracy: 47.66%\n",
      "low tensor(0., device='cuda:0') tensor(1., device='cuda:0')\n",
      "delta tensor(-0.0314, device='cuda:0') tensor(0.0314, device='cuda:0')\n",
      "db rob acc  0.484375 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test_robust_acc 0.484375 0.484375 clean 0.8203125\n",
      "using standard version including apgd-ce, apgd-t, fab-t, square\n",
      "initial accuracy: 81.25%\n",
      "apgd-ce - 1/1 - 41 out of 104 successfully perturbed\n",
      "robust accuracy after APGD-CE: 49.22% (total time 165.3 s)\n",
      "apgd-t - 1/1 - 2 out of 63 successfully perturbed\n",
      "robust accuracy after APGD-T: 47.66% (total time 742.6 s)\n",
      "fab-t - 1/1 - 0 out of 61 successfully perturbed\n",
      "robust accuracy after FAB-T: 47.66% (total time 1475.4 s)\n",
      "square - 1/1 - 0 out of 61 successfully perturbed\n",
      "robust accuracy after SQUARE: 47.66% (total time 2330.8 s)\n",
      "max Linf perturbation: 0.03137, nan in tensor: 0, max: 1.00000, min: 0.00000\n",
      "robust accuracy: 47.66%\n",
      "low tensor(0., device='cuda:0') tensor(1., device='cuda:0')\n",
      "delta tensor(-0.0314, device='cuda:0') tensor(0.0314, device='cuda:0')\n",
      "db rob acc  0.4765625 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test_robust_acc 0.48046875 0.48046875 clean 0.81640625\n",
      "using standard version including apgd-ce, apgd-t, fab-t, square\n",
      "initial accuracy: 80.47%\n",
      "apgd-ce - 1/1 - 38 out of 103 successfully perturbed\n",
      "robust accuracy after APGD-CE: 50.78% (total time 166.9 s)\n",
      "apgd-t - 1/1 - 2 out of 65 successfully perturbed\n",
      "robust accuracy after APGD-T: 49.22% (total time 756.7 s)\n",
      "fab-t - 1/1 - 0 out of 63 successfully perturbed\n",
      "robust accuracy after FAB-T: 49.22% (total time 1506.4 s)\n",
      "square - 1/1 - 0 out of 63 successfully perturbed\n",
      "robust accuracy after SQUARE: 49.22% (total time 2391.5 s)\n",
      "max Linf perturbation: 0.03137, nan in tensor: 0, max: 1.00000, min: 0.00000\n",
      "robust accuracy: 49.22%\n",
      "low tensor(0., device='cuda:0') tensor(1., device='cuda:0')\n",
      "delta tensor(-0.0314, device='cuda:0') tensor(0.0314, device='cuda:0')\n",
      "db rob acc  0.4921875 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test_robust_acc 0.484375 0.484375 clean 0.8125\n",
      "using standard version including apgd-ce, apgd-t, fab-t, square\n",
      "initial accuracy: 83.59%\n",
      "apgd-ce - 1/1 - 36 out of 107 successfully perturbed\n",
      "robust accuracy after APGD-CE: 55.47% (total time 174.1 s)\n",
      "apgd-t - 1/1 - 6 out of 71 successfully perturbed\n",
      "robust accuracy after APGD-T: 50.78% (total time 785.2 s)\n",
      "fab-t - 1/1 - 0 out of 65 successfully perturbed\n",
      "robust accuracy after FAB-T: 50.78% (total time 1559.5 s)\n",
      "square - 1/1 - 0 out of 65 successfully perturbed\n",
      "robust accuracy after SQUARE: 50.78% (total time 2473.5 s)\n",
      "max Linf perturbation: 0.03137, nan in tensor: 0, max: 1.00000, min: 0.00000\n",
      "robust accuracy: 50.78%\n",
      "low tensor(0., device='cuda:0') tensor(1., device='cuda:0')\n",
      "delta tensor(-0.0314, device='cuda:0') tensor(0.0314, device='cuda:0')\n",
      "db rob acc  0.5078125 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test_robust_acc 0.490234375 0.490234375 clean 0.818359375\n",
      "using standard version including apgd-ce, apgd-t, fab-t, square\n",
      "initial accuracy: 83.59%\n",
      "apgd-ce - 1/1 - 33 out of 107 successfully perturbed\n",
      "robust accuracy after APGD-CE: 57.81% (total time 175.1 s)\n",
      "apgd-t - 1/1 - 2 out of 74 successfully perturbed\n",
      "robust accuracy after APGD-T: 56.25% (total time 820.5 s)\n",
      "fab-t - 1/1 - 0 out of 72 successfully perturbed\n",
      "robust accuracy after FAB-T: 56.25% (total time 1645.0 s)\n",
      "square - 1/1 - 0 out of 72 successfully perturbed\n",
      "robust accuracy after SQUARE: 56.25% (total time 2634.4 s)\n",
      "max Linf perturbation: 0.03137, nan in tensor: 0, max: 1.00000, min: 0.00000\n",
      "robust accuracy: 56.25%\n",
      "low tensor(0., device='cuda:0') tensor(1., device='cuda:0')\n",
      "delta tensor(-0.0314, device='cuda:0') tensor(0.0314, device='cuda:0')\n",
      "db rob acc  0.5625 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test_robust_acc 0.5046875 0.5046875 clean 0.821875\n",
      "using standard version including apgd-ce, apgd-t, fab-t, square\n",
      "initial accuracy: 79.69%\n",
      "apgd-ce - 1/1 - 41 out of 102 successfully perturbed\n",
      "robust accuracy after APGD-CE: 47.66% (total time 162.4 s)\n",
      "apgd-t - 1/1 - 3 out of 61 successfully perturbed\n",
      "robust accuracy after APGD-T: 45.31% (total time 709.7 s)\n",
      "fab-t - 1/1 - 0 out of 58 successfully perturbed\n",
      "robust accuracy after FAB-T: 45.31% (total time 1401.2 s)\n",
      "square - 1/1 - 0 out of 58 successfully perturbed\n",
      "robust accuracy after SQUARE: 45.31% (total time 2214.6 s)\n",
      "max Linf perturbation: 0.03137, nan in tensor: 0, max: 1.00000, min: 0.00000\n",
      "robust accuracy: 45.31%\n",
      "low tensor(0., device='cuda:0') tensor(1., device='cuda:0')\n",
      "delta tensor(-0.0314, device='cuda:0') tensor(0.0314, device='cuda:0')\n",
      "db rob acc  0.453125 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test_robust_acc 0.49609375 0.49609375 clean 0.8177083333333334\n",
      "using standard version including apgd-ce, apgd-t, fab-t, square\n",
      "initial accuracy: 78.12%\n",
      "apgd-ce - 1/1 - 33 out of 100 successfully perturbed\n",
      "robust accuracy after APGD-CE: 52.34% (total time 164.6 s)\n",
      "apgd-t - 1/1 - 5 out of 67 successfully perturbed\n",
      "robust accuracy after APGD-T: 48.44% (total time 748.6 s)\n",
      "fab-t - 1/1 - 0 out of 62 successfully perturbed\n",
      "robust accuracy after FAB-T: 48.44% (total time 1486.5 s)\n",
      "square - 1/1 - 0 out of 62 successfully perturbed\n",
      "robust accuracy after SQUARE: 48.44% (total time 2348.3 s)\n",
      "max Linf perturbation: 0.03137, nan in tensor: 0, max: 1.00000, min: 0.00000\n",
      "robust accuracy: 48.44%\n",
      "low tensor(0., device='cuda:0') tensor(1., device='cuda:0')\n",
      "delta tensor(-0.0314, device='cuda:0') tensor(0.0314, device='cuda:0')\n",
      "db rob acc  0.484375 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test_robust_acc 0.49441964285714285 0.49441964285714285 clean 0.8125\n",
      "using standard version including apgd-ce, apgd-t, fab-t, square\n",
      "initial accuracy: 82.81%\n",
      "apgd-ce - 1/1 - 37 out of 106 successfully perturbed\n",
      "robust accuracy after APGD-CE: 53.91% (total time 171.9 s)\n",
      "apgd-t - 1/1 - 4 out of 69 successfully perturbed\n",
      "robust accuracy after APGD-T: 50.78% (total time 782.2 s)\n",
      "fab-t - 1/1 - 0 out of 65 successfully perturbed\n",
      "robust accuracy after FAB-T: 50.78% (total time 1557.1 s)\n",
      "square - 1/1 - 0 out of 65 successfully perturbed\n",
      "robust accuracy after SQUARE: 50.78% (total time 2472.0 s)\n",
      "max Linf perturbation: 0.03137, nan in tensor: 0, max: 1.00000, min: 0.00000\n",
      "robust accuracy: 50.78%\n",
      "low tensor(0., device='cuda:0') tensor(1., device='cuda:0')\n",
      "delta tensor(-0.0314, device='cuda:0') tensor(0.0314, device='cuda:0')\n",
      "db rob acc  0.5078125 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test_robust_acc 0.49609375 0.49609375 clean 0.814453125\n",
      "using standard version including apgd-ce, apgd-t, fab-t, square\n",
      "initial accuracy: 78.12%\n",
      "apgd-ce - 1/1 - 35 out of 100 successfully perturbed\n",
      "robust accuracy after APGD-CE: 50.78% (total time 163.8 s)\n",
      "apgd-t - 1/1 - 4 out of 65 successfully perturbed\n",
      "robust accuracy after APGD-T: 47.66% (total time 747.7 s)\n",
      "fab-t - 1/1 - 0 out of 61 successfully perturbed\n",
      "robust accuracy after FAB-T: 47.66% (total time 1480.3 s)\n",
      "square - 1/1 - 0 out of 61 successfully perturbed\n",
      "robust accuracy after SQUARE: 47.66% (total time 2334.1 s)\n",
      "max Linf perturbation: 0.03137, nan in tensor: 0, max: 1.00000, min: 0.00000\n",
      "robust accuracy: 47.66%\n",
      "low tensor(0., device='cuda:0') tensor(1., device='cuda:0')\n",
      "delta tensor(-0.0314, device='cuda:0') tensor(0.0314, device='cuda:0')\n",
      "db rob acc  0.4765625 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test_robust_acc 0.4939236111111111 0.4939236111111111 clean 0.8107638888888888\n",
      "using standard version including apgd-ce, apgd-t, fab-t, square\n",
      "initial accuracy: 82.03%\n",
      "apgd-ce - 1/1 - 36 out of 105 successfully perturbed\n",
      "robust accuracy after APGD-CE: 53.91% (total time 170.3 s)\n",
      "apgd-t - 1/1 - 5 out of 69 successfully perturbed\n",
      "robust accuracy after APGD-T: 50.00% (total time 772.3 s)\n",
      "fab-t - 1/1 - 0 out of 64 successfully perturbed\n",
      "robust accuracy after FAB-T: 50.00% (total time 1532.8 s)\n",
      "square - 1/1 - 0 out of 64 successfully perturbed\n",
      "robust accuracy after SQUARE: 50.00% (total time 2420.9 s)\n",
      "max Linf perturbation: 0.03137, nan in tensor: 0, max: 1.00000, min: 0.00000\n",
      "robust accuracy: 50.00%\n",
      "low tensor(0., device='cuda:0') tensor(1., device='cuda:0')\n",
      "delta tensor(-0.0314, device='cuda:0') tensor(0.0314, device='cuda:0')\n",
      "db rob acc  0.5 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test_robust_acc 0.49453125 0.49453125 clean 0.81171875\n",
      "clean contrastive=6.350154 \t adv contrastive=6.209636\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "torch.save(TestX, \"TestX.pt\")\n",
    "torch.save(TestY, \"TestY.pt\")\n",
    "torch.save(Testdelta, \"Testdelta.pt\")\n",
    "print(contrastive_attack_loss, contrastive_clean_loss)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7948.3343505859375 8128.197265625\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "TestX = torch.load(\"TestX.pt\")\n",
    "TestY = torch.load(\"TestY.pt\")\n",
    "Testdelta = torch.load( \"Testdelta.pt\")\n",
    "# TestX = TestX[11:]\n",
    "# TestY = TestY[11:]\n",
    "# Testdelta = Testdelta[11:]\n",
    "contrastive_attack_loss = 7948.3343505859375\n",
    "contrastive_clean_loss = 8128.197265625"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "(test_robust_ada_acc, test_clean_ada_acc,test_n)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(783, 1024, 1280)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "((test_loss), (test_acc), (test_clean_ada_acc / test_n * 100))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0, 0, 80.0)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "torch.save(test_robust_ada_acc, \"test_robust_ada_acc.pt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "TestX = torch.cat(TestX, dim=0)\n",
    "TestY = torch.cat(TestY, dim=0)\n",
    "Testdelta = torch.cat(Testdelta, dim=0)\n",
    "print(TestX.size(), TestY.size(), Testdelta.size())\n",
    "\n",
    "total_len = TestX.shape[0]\n",
    "ind = [i for i in range(total_len)]\n",
    "from random import shuffle\n",
    "shuffle(ind)\n",
    "# TestX = TestX[ind]\n",
    "# TestY = TestY[ind]\n",
    "# Testdelta = Testdelta[ind]\n",
    "\n",
    "bs = 64 #args.contrastive_bs\n",
    "\n",
    "num_bs = TestX.size(0) // bs\n",
    "if num_bs * bs < TestX.size(0):\n",
    "    num_bs += 1\n",
    "count_test = 0\n",
    "print(num_bs)\n",
    "Base_atack_steps = [20]\n",
    "for base_attack_step in Base_atack_steps:\n",
    "    for adda_times in [2]:\n",
    "        test_robust_ada_acc = 0\n",
    "        test_clean_ada_acc = 0\n",
    "        test_robust_ada_loss = 0\n",
    "        test_clean_ada_loss = 0\n",
    "        # test_clean_loss = 0\n",
    "        # test_clean_acc = 0\n",
    "\n",
    "        test_n = 0\n",
    "        adaadv_contrastive_loss=0\n",
    "\n",
    "        print('contrastive bs', bs)\n",
    "\n",
    "        for bs_ind in range(num_bs):\n",
    "            print(bs_ind)\n",
    "            if args.debug and bs_ind > 0:\n",
    "                break\n",
    "            model.module.set_precision(num_bits=np.random.choice(num_bits_list), num_grad_bits=0)\n",
    "            X = TestX[bs_ind * bs:(bs_ind + 1) * bs]\n",
    "            y = TestY[bs_ind * bs:(bs_ind + 1) * bs]\n",
    "            delta = Testdelta[bs_ind * bs:(bs_ind + 1) * bs]\n",
    "\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "            delta = delta.cuda()\n",
    "\n",
    "            # Random initialization\n",
    "            if args.random_noise:\n",
    "                delta2 = torch.zeros_like(X)\n",
    "                delta2.uniform_(-epsilon* adda_times, epsilon* adda_times)\n",
    "            else:\n",
    "                delta2 = attack_constrastive_Mhead(model, c_head_model, scripted_transforms, criterion,\n",
    "                                                   torch.clamp(X + delta[:X.size(0)], min=lower_limit, max=upper_limit),\n",
    "                                                   torch.zeros_like(y), epsilon * adda_times, pgd_alpha,  # 1, 0.2,\n",
    "                                                   int(base_attack_step * adda_times) if not args.rand else 0,\n",
    "                                                   args.restarts, args.norm, n_views=args.n_views\n",
    "                                                   )\n",
    "                delta2 = delta2.detach()\n",
    "            model.module.set_precision(num_bits=np.random.choice(num_bits_list), num_grad_bits=0)\n",
    "            robust_output_ada, hidden = model(\n",
    "                normalize(torch.clamp(X + delta[:X.size(0)], min=lower_limit, max=upper_limit) + delta2))\n",
    "            test_robust_ada_acc += (robust_output_ada.max(1)[1] == y).sum().item()\n",
    "            robust_ada_loss = criterion(robust_output_ada, y)\n",
    "            test_robust_ada_loss += robust_ada_loss.item() * y.size(0)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            contrastive_ada_attack = \\\n",
    "                calculate_contrastive_Mhead_loss(\n",
    "                    torch.clamp(\n",
    "                        torch.clamp(X + delta[:X.size(0)], min=lower_limit, max=upper_limit) + delta2,\n",
    "                        min=lower_limit, max=upper_limit),\n",
    "                    scripted_transforms, model, criterion, c_head_model)\n",
    "            adaadv_contrastive_loss += contrastive_ada_attack.item() * y.size(0)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if args.random_noise:\n",
    "                delta3 = delta2\n",
    "            else:\n",
    "                delta3 = attack_constrastive_Mhead(model, c_head_model, scripted_transforms,\n",
    "                                                   criterion,\n",
    "                                                   X,\n",
    "                                                   torch.zeros_like(y), epsilon * adda_times, pgd_alpha,  # 1, 0.2,\n",
    "                                                   int(base_attack_step * adda_times) if not args.rand else 0,\n",
    "                                                   args.restarts, args.norm,\n",
    "                                                   early_stop=args.eval, n_views=args.n_views)\n",
    "                #     #epsilon * args.adda_times, pgd_alpha\n",
    "                delta3 = delta3.detach()\n",
    "\n",
    "            clean_output_ada, hidden = model(\n",
    "                normalize(X + delta3))\n",
    "            test_clean_ada_acc += (clean_output_ada.max(1)[1] == y).sum().item()\n",
    "\n",
    "            clean_ada_loss = criterion(clean_output_ada, y)\n",
    "            test_clean_ada_loss += clean_ada_loss.item() * y.size(0)\n",
    "            test_n += y.size(0)\n",
    "            # print(test_robust_ada_acc, test_robust_ada_loss)\n",
    "            print(\n",
    "            'e=%d  scale=%.4f step=%d epsilon=%d \\t TestLoss=%.4f TestAcc=%.4f TestCleanAdaAcc=%.4f \\t TestRobLoss=%.4f TestRobAcc %.4f \\t AdaTestLoss=%.4f AdaTestAcc %.4f' %\n",
    "            (0, adda_times, (int(base_attack_step * adda_times)), (epsilon*255),\n",
    "             (test_loss / test_n), (test_acc / test_n * 100), (test_clean_ada_acc / test_n * 100),\n",
    "             (test_robust_loss / test_n), (test_robust_acc / test_n * 100),\n",
    "             (test_robust_loss / test_n), (test_robust_ada_acc / test_n * 100)))\n",
    "            torch.cuda.empty_cache()\n",
    "            # print(bs_ind)\n",
    "\n",
    "        print(\"Final: \" +  \n",
    "            'e=%d  scale=%.4f step=%d epsilon=%d \\t TestLoss=%.4f TestAcc=%.4f TestCleanAdaAcc=%.4f \\t TestRobLoss=%.4f TestRobAcc %.4f \\t AdaTestLoss=%.4f AdaTestAcc %.4f' %\n",
    "            (0, adda_times, (int(base_attack_step * adda_times)), (epsilon*255),\n",
    "             (test_loss / test_n), (test_acc / test_n * 100), (test_clean_ada_acc / test_n * 100),\n",
    "             (test_robust_loss / test_n), (test_robust_acc / test_n * 100),\n",
    "             (test_robust_loss / test_n), (test_robust_ada_acc / test_n * 100)))\n",
    "        print('clean contrastive=%.6f \\t adv contrastive=%.6f \\t adaadv contrastive=%.6f' %\n",
    "              ((contrastive_clean_loss / test_n), (contrastive_attack_loss / test_n),\n",
    "               (adaadv_contrastive_loss / test_n)))\n",
    "print(\"\\n\\n\\nmogu\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1280, 3, 32, 32]) torch.Size([1280]) torch.Size([1280, 3, 32, 32])\n",
      "20\n",
      "contrastive bs 64\n",
      "0\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=73.4375 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 53.1250\n",
      "1\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=79.6875 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 58.5938\n",
      "2\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=81.2500 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 60.9375\n",
      "3\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=82.0312 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 60.9375\n",
      "4\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=80.3125 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 59.3750\n",
      "5\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=80.4688 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 60.1562\n",
      "6\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=80.1339 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 60.4911\n",
      "7\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=79.8828 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 60.5469\n",
      "8\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=79.8611 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 61.1111\n",
      "9\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=80.7812 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 61.4062\n",
      "10\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=80.2557 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 60.9375\n",
      "11\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=80.3385 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 60.5469\n",
      "12\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=80.0481 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 60.4567\n",
      "13\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=79.5759 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 60.6027\n",
      "14\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=79.8958 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 60.6250\n",
      "15\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=80.1758 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 61.2305\n",
      "16\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=80.2390 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 61.2132\n",
      "17\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=80.0347 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 60.7639\n",
      "18\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=80.2632 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 61.5132\n",
      "19\n",
      "e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=80.0000 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 61.1719\n",
      "Final: e=0  scale=2.0000 step=40 epsilon=8 \t TestLoss=0.0000 TestAcc=0.0000 TestCleanAdaAcc=80.0000 \t TestRobLoss=0.0000 TestRobAcc 0.0000 \t AdaTestLoss=0.0000 AdaTestAcc 61.1719\n",
      "clean contrastive=6.350154 \t adv contrastive=6.209636 \t adaadv contrastive=4.774158\n",
      "\n",
      "\n",
      "\n",
      "mogu\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t,r,a,f)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "17071734784 0 0 0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCS Python",
   "language": "python",
   "name": "cscs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}